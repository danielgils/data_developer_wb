
A.	Would you store data in databricks as delta or parquet format? Based on your choice, please explain why? 

Taking into account that Delta is an extension of the Parquet format, and that there are almost no downsides in using Delta over Parquet, I would store the data in Delta format. However, this decision depends on the use case and requirements of the data that is going to be stored. 

If the use case consists only in data processing and analytics (without having to worry about ACID transactions or versioning) I would use Parquet format, because it keeps things simple and in the case that new capabilities are needed the migration to Delta is straightforward (Although it may take some processing time to get the metadata, and get current and past states of the data). On the other hand, if the use case involves transactional data or there is a need to keep different versions of the data, then it would be better to use Delta format. This is mainly because of the main features that Delta format offers over Parquet: ACID transactions, data versioning, transaction logs, checkpoint files, among others.  

Actually this question reminded me of a project I was working as a data scientist at the end of last year. We were building a recommendation system on top of a datalake hosted in AWS using parquet files, and by the end of the Decemeber we had to stop the project because the data architects of the company decided to make the migration from Parquet to Delta because of the nature of the data we were handling and the needs they were having in regards to consistency and versioning.

________________________________________________________________________________________________________________________________________________
B.	Please explain the process of how you would optimize PySpark or SQL code to effectively use databricks spark cluster?

The process to optimize PySpark or SQL code requires a combination of planning, experimentation and on going monitoring. With respect to planning, it is key to decide how the data is going to be partitioned and what data formats are going to be used. Partitioning data can help to distribute workloads across the cluster, althought it is important to consider the size and shape of the data, as well as the specific queries or transformations that will be performed on it. Using data formats that are efficient to work in cluster, such as Parquet or Delta, results in better performance and reduced resource usage.

Regarding experimentation, caching and persisting data, and optimizing resource allocation are important steps to consider. Caching and persisting data can help to reduce the amount of time required to read data from disk or perform calculations, although it is important to use them carefully because they can consume significant amounts of memory and lead to increased resource usage. On the other hand, it is important to allocate resources appropriately in the different stages of the data processing pipeline. This can be achieved by using dynamic allocation, which allows the cluster to allocate resources based on the current workload, or by adjusting the configuration settings for Spark executor and driver memory.

Finally, on going monitoring based on performance metrics and diagnostics can help to discover performance bottlenecks and areas for optimization. This optimization can be related to the steps mentioned before. 