
A.	Would you store data in databricks as delta or parquet format? Based on your choice, please explain why? 

Taking into account that Delta is an extension of the Parquet format, and that there are almost no downsides to using Delta over Parquet, I would store the data in Delta format. However, this decision depends on the use case and requirements of the data being stored.

If the use case consists only in data processing and analytics (without having to worry about ACID transactions or versioning), I would use Parquet format because it keeps things simple. In the event that new capabilities are needed, migration to Delta is straightforward, although it may take some processing time to obtain metadata and retrieve past and current data states. On the other hand, if the use case involves transactional data or there is a need to keep different versions of the data, then it would be better to use Delta format. This is mainly because of the main features that Delta format offers over Parquet, such as ACID transactions, data versioning, transaction logs, checkpoint files, among others.  

Actually, this question reminded me of a project I worked on as a data scientist at the end of last year. We were building a recommendation system on top of a datalake hosted in AWS using parquet files. However, by the end of December, we had to halt the project because the data architects of the company decided to migrate from Parquet to Delta due to the nature of the data we were handling and the need for consistency and versioning.

________________________________________________________________________________________________________________________________________________
B.	Please explain the process of how you would optimize PySpark or SQL code to effectively use databricks spark cluster?

Optimizing PySpark or SQL code involves a combination of planning, experimentation, and ongoing monitoring. Planning is critical in determining how the data will be partitioned and what data formats will be used. Partitioning data can help to distribute workloads across the cluster, but it is important to consider the size and shape of the data, as well as the specific queries or transformations that will be performed. On the other hand, using efficient data formats, such as Parquet or Delta, can result in better performance and reduced resource usage.

When it comes to experimentation, caching and persisting data, and optimizing resource allocation are important steps. Caching and persisting data can reduce the time needed to read data from disk or perform calculations. However, it is crucial to use them carefully because they can consume significant amounts of memory and increase resource usage. Additionally, it is important to allocate resources appropriately in different stages of the data processing pipeline. This can be achieved by using dynamic allocation, which allows the cluster to allocate resources based on the current workload, or by adjusting the configuration settings for Spark executor and driver memory.

Finally, ongoing monitoring based on performance metrics and diagnostics can help to identify performance bottlenecks and areas for optimization, including those areas related to planning and experimentation mentioned before.